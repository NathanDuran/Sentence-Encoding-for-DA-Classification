# Supervised and Semi-supervised Sentence Encoding Methods for Dialogue Act Classification

# TODO

## models
- CRF?
- Test average pooling and time distributed (av pooling seems better)

- GRU?
- Recurrent convolutional - Ribeiro, E., Ribeiro, R. and de Matos, D. M. (2018) ‘Deep Dialog Act Recognition using Multiple Token, Segment, and Context Information Representations’, arXiv. doi: arXiv:1807.08587v1.

- rnnlm + char level language model
- mLSTM https://github.com/titu1994/Keras-Multiplicative-LSTM

- bert Different num layers? 

## Misc
- Add truncating word vectors to embedding dim
- Change test to log final values and step values
- Save training history
- Document models in readme

## To try
- Visualising attention layers
- Saving trained embeddings
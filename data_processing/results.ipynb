{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import collections\n",
    "from urllib.request import urlopen\n",
    "from data_processing import *\n",
    "pd.options.display.width = 0\n",
    "#%matplotlib inline\n",
    "\n",
    "# Set the task and experiment type\n",
    "task_name = 'swda'\n",
    "base_url = 'https://raw.github.com/NathanDuran/Switchboard-Corpus/master/swda_data/'\n",
    "metadata = pickle.load(urlopen(base_url + 'metadata/metadata.pkl'))\n",
    "text_data = [line.decode('utf-8').rstrip('\\r\\n') for line in urlopen(base_url + 'train_set.txt')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Vocabulary Size\n",
    "The number of words to keep in the vocabulary during pre-processing. Increments of 500 in the range [500, 8000].\n",
    "\n",
    "Get the vocabulary size corresponding to max validation and test accuracy per model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Name of the experiment parameter for comparison\n",
    "exp_param = 'vocab_size'\n",
    "\n",
    "# Load vocabulary experiment data\n",
    "vocab_data = load_dataframe(os.path.join(task_name, exp_param, exp_param + '_data.csv'))\n",
    "vocab_mean_data = load_dataframe(os.path.join(task_name, exp_param, exp_param + '_mean_data.csv'))\n",
    "\n",
    "# Get the max values for each model\n",
    "print(\"Best validation accuracy in mean data:\")\n",
    "print(vocab_mean_data.loc[[vocab_mean_data['val_acc'].idxmax()], ['model_name', exp_param, 'val_acc']])\n",
    "print(\"Best test accuracy in mean data:\")\n",
    "print(vocab_mean_data.loc[[vocab_mean_data['test_acc'].idxmax()], ['model_name', exp_param, 'test_acc', 'f1_micro']])\n",
    "vocab_data_max = get_max(vocab_mean_data, [exp_param])\n",
    "vocab_data_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot accuracy of each model per vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO remove test data\n",
    "# Get test and validation accuracy for each model\n",
    "# vocab_acc_data = vocab_data.drop(vocab_data.columns.difference(['model_name', exp_param, 'val_acc', 'test_acc']), axis=1)\n",
    "# vocab_acc_data = vocab_acc_data.rename(columns={'val_acc': 'Val Acc', 'test_acc': 'Test Acc'})\n",
    "# vocab_acc_data = vocab_acc_data.melt(id_vars=['model_name', exp_param])\n",
    "\n",
    "# g, fig = plot_relplot(vocab_acc_data, x=exp_param, y='value', hue='model_name', col='variable', kind='line', ci='sd',\n",
    "#                       title='', y_label='Accuracy', x_label='Vocabulary Size',  share_x=False, share_y=False, num_col=1,\n",
    "#                       legend_loc='lower right', num_legend_col=4, colour='Paired')\n",
    "# g.savefig(os.path.join(task_name, exp_param ,exp_param + '_accuracy.png'))\n",
    "\n",
    "# Get word frequencies from metadata and bin into same values as experiment\n",
    "word_freq = metadata['word_freq'].iloc[:8000]\n",
    "ind_start = 0\n",
    "ind_end = 500\n",
    "freq_dict = dict()\n",
    "for i in range(16):\n",
    "    freq_dict[ind_end] = word_freq.loc[ind_start:ind_end].mean()\n",
    "    ind_start += 500\n",
    "    ind_end += 500\n",
    "word_freq = pd.DataFrame.from_dict(freq_dict, orient='index')\n",
    "word_freq.reset_index(level=0, inplace=True)\n",
    "word_freq.rename(columns={'index': 'vocab_size'}, inplace=True)\n",
    "\n",
    "# Get experiment validation accuracy data for each model\n",
    "vocab_acc_data = vocab_data.drop(vocab_data.columns.difference(['model_name', exp_param, 'val_acc']), axis=1)\n",
    "vocab_acc_data = vocab_acc_data.rename(columns={'val_acc': 'Val Acc'})\n",
    "vocab_acc_data = vocab_acc_data.melt(id_vars=['model_name', exp_param])\n",
    "\n",
    "# Plot line/bar chart\n",
    "fig = plot_line_bar_chart(vocab_acc_data, word_freq, x=exp_param, y='value', hue='model_name',\n",
    "                          title='', y_label='Val Acc', x_label='Vocabulary Size', colour='Paired',\n",
    "                          bar_x=exp_param, bar_y='Count', bar_axis_step=20, bar_axis_range=[0, 250], bar_width=100,\n",
    "                          bar_y_label='Mean Word Freq', bar_alpha=0.5, bar_color='tab:blue', legend_loc='best', num_legend_col=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test ANOVA assumptions using Shapiro-Wilks test for normality and Levene test for equal variance (Homoscedasticity)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the metric to test\n",
    "metric = 'val_acc'\n",
    "\n",
    "# Run Shaprio-wilks for all models and vocabulary sizes\n",
    "shapiro_wilk_test(vocab_data, exp_param, metric)\n",
    "\n",
    "# Run Levene test for equal variance\n",
    "levene_test(vocab_data, exp_param, metric)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform ANOVA for each model, comparing different vocabulary size groups,\n",
    "followed by Tukey Honest Significant Difference Post-Hoc analysis for pairwise comparison of groups.\n",
    "\n",
    "Heatmaps shows Tukey-HSD results for each model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run ANOVA\n",
    "one_way_anova_test(vocab_data, exp_param, metric)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run Tukey-HSD post-hoc analysis\n",
    "tukey_frame = tukey_hsd(vocab_data, exp_param, metric, show_result=False)\n",
    "\n",
    "# Drop the un-needed columns and generate heatmaps\n",
    "tukey_frame = tukey_frame.drop(columns=['meandiff', 'lower', 'upper', 'reject'], axis=1)\n",
    "# Remove vocab_size > 5000 to make plots nicer\n",
    "tukey_frame.drop(tukey_frame[(tukey_frame.group1 > 5000) | (tukey_frame.group2 > 5000)].index, inplace=True)\n",
    "\n",
    "g, fig = plot_facetgrid(tukey_frame, x='group1', y='group2', hue='p-value', col='model_name', kind='heatmap',\n",
    "                        title='', y_label='', x_label='', num_col=2, colour='RdBu_r',\n",
    "                        annot=True, fmt='0.3', linewidths=0.5, cbar=False, custom_boundaries=[0.0, 0.05, 1.0],\n",
    "                        y_tick_rotation=45, height=4)\n",
    "# g.savefig(os.path.join(task_name, exp_param ,exp_param + '_anova.png'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sequence Length\n",
    "The number of tokens in the input sequence. Sentences are padded or truncated to this length.\n",
    "Increments of 5 in the range [5, 50].\n",
    "\n",
    "Get the sequence length corresponding to max validation and test accuracy per model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Name of the experiment parameter for comparison\n",
    "exp_param = 'max_seq_length'\n",
    "\n",
    "# Load sequence length experiment data\n",
    "seq_data = load_dataframe(os.path.join(task_name, exp_param, exp_param + '_data.csv'))\n",
    "seq_mean_data = load_dataframe(os.path.join(task_name, exp_param, exp_param + '_mean_data.csv'))\n",
    "\n",
    "# Get the max values for each model\n",
    "print(\"Best validation accuracy in mean data:\")\n",
    "print(seq_mean_data.loc[[seq_mean_data['val_acc'].idxmax()], ['model_name', exp_param, 'val_acc']])\n",
    "print(\"Best test accuracy in mean data:\")\n",
    "print(seq_mean_data.loc[[seq_mean_data['test_acc'].idxmax()], ['model_name', exp_param, 'test_acc', 'f1_micro']])\n",
    "seq_data_max = get_max(seq_mean_data, [exp_param])\n",
    "seq_data_max"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot accuracy of each model per sequence length."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO remove test data\n",
    "# Get test and validation accuracy for each model\n",
    "# seq_acc_data = seq_data.drop(seq_data.columns.difference(['model_name', exp_param, 'val_acc', 'test_acc']), axis=1)\n",
    "# seq_acc_data = seq_acc_data.rename(columns={'val_acc': 'Val Acc', 'test_acc': 'Test Acc'})\n",
    "# seq_acc_data = seq_acc_data.melt(id_vars=['model_name', exp_param])\n",
    "#\n",
    "# g, fig = plot_relplot(seq_acc_data, x=exp_param, y='value', hue='model_name', col='variable', kind='line', ci='sd',\n",
    "#                       title='', y_label='Accuracy', x_label='Sequence Length',  share_x=False, share_y=False, num_col=1,\n",
    "#                       legend_loc='lower right', num_legend_col=4, colour='Paired')\n",
    "# g.savefig(os.path.join(task_name, exp_param ,exp_param + '_accuracy.png'))\n",
    "\n",
    "# Get text data from training set and count sequence lengths\n",
    "sentences = [line.split('|')[1] for line in text_data]\n",
    "sent_lengths = [len(sent.split(' ')) for sent in sentences]\n",
    "sent_lengths = collections.Counter(sent_lengths)\n",
    "sent_lengths = pd.DataFrame.from_dict(dict(sorted(sent_lengths.items())), orient='index')\n",
    "\n",
    "# Bin into same values as experiment\n",
    "ind_start = 0\n",
    "ind_end = 5\n",
    "freq_dict = dict()\n",
    "for i in range(10):\n",
    "    freq_dict[ind_end] = sent_lengths.loc[ind_start:ind_end].sum()\n",
    "    ind_start += 5\n",
    "    ind_end += 5\n",
    "sent_lengths = pd.DataFrame.from_dict(freq_dict, orient='index')\n",
    "sent_lengths.reset_index(level=0, inplace=True)\n",
    "sent_lengths.rename(columns={'index': 'max_seq_length', 0: 'Count'}, inplace=True)\n",
    "\n",
    "# Get experiment validation accuracy data for each model\n",
    "seq_acc_data = seq_data.drop(seq_data.columns.difference(['model_name', exp_param, 'val_acc']), axis=1)\n",
    "seq_acc_data = seq_acc_data.rename(columns={'val_acc': 'Val Acc'})\n",
    "seq_acc_data = seq_acc_data.melt(id_vars=['model_name', exp_param])\n",
    "\n",
    "# Plot line/bar chart\n",
    "fig = plot_line_bar_chart(seq_acc_data, sent_lengths, x=exp_param, y='value', hue='model_name',\n",
    "                          title='', y_label='Val Acc', x_label='Max Sequence Length', colour='Paired',\n",
    "                          bar_x=exp_param, bar_y='Count', bar_axis_step=5000, bar_axis_range=None, bar_width=1,\n",
    "                          bar_y_label='Num Sentences', bar_alpha=0.5, bar_color='tab:blue', legend_loc='lower right', num_legend_col=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test ANOVA assumptions using Shapiro-Wilks test for normality and Levene test for equal variance (Homoscedasticity)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the metric to test\n",
    "metric = 'val_acc'\n",
    "\n",
    "# Run Shaprio-wilks for all models and sequence lengths\n",
    "shapiro_wilk_test(seq_data, exp_param, metric)\n",
    "\n",
    "# Run Levene test for equal variance\n",
    "levene_test(seq_data, exp_param, metric)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform ANOVA for each model, comparing different sequence length groups,\n",
    "followed by Tukey Honest Significant Difference Post-Hoc analysis for pairwise comparison of groups.\n",
    "\n",
    "Heatmaps shows Tukey-HSD results for each model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run ANOVA\n",
    "one_way_anova_test(seq_data, exp_param, metric)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run Tukey-HSD post-hoc analysis\n",
    "tukey_frame = tukey_hsd(seq_data, exp_param, metric, show_result=False)\n",
    "\n",
    "# Drop the un-needed columns and generate heatmaps\n",
    "tukey_frame = tukey_frame.drop(columns=['meandiff', 'lower', 'upper', 'reject'], axis=1)\n",
    "\n",
    "g, fig = plot_facetgrid(tukey_frame, x='group1', y='group2', hue='p-value', col='model_name', kind='heatmap',\n",
    "                        title='', y_label='', x_label='', num_col=2, colour='RdBu_r',\n",
    "                        annot=True, fmt='0.3', linewidths=0.5, cbar=False, custom_boundaries=[0.0, 0.05, 1.0],\n",
    "                        y_tick_rotation=45, height=4)\n",
    "# g.savefig(os.path.join(task_name, exp_param ,exp_param + '_anova.png'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use Punctuation\n",
    "Whether to remove punctuation from the input sentences or not.\n",
    "\n",
    "Show punctuation flag value corresponding to max validation and test accuracy per model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Name of the experiment parameter for comparison\n",
    "exp_param = 'use_punct'\n",
    "\n",
    "# Load punctuation experiment data\n",
    "punc_data = load_dataframe(os.path.join(task_name, exp_param, exp_param + '_data.csv'))\n",
    "punc_mean_data = load_dataframe(os.path.join(task_name, exp_param, exp_param + '_mean_data.csv'))\n",
    "\n",
    "# Get the max values for each model\n",
    "print(\"Best validation accuracy in mean data:\")\n",
    "print(punc_mean_data.loc[[punc_mean_data['val_acc'].idxmax()], ['model_name', exp_param, 'val_acc']])\n",
    "print(\"Best test accuracy in mean data:\")\n",
    "print(punc_mean_data.loc[[punc_mean_data['test_acc'].idxmax()], ['model_name', exp_param, 'test_acc', 'f1_micro']])\n",
    "punc_data_max = get_max(punc_mean_data, [exp_param])\n",
    "punc_data_max"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot accuracy of each model per punctuation flag value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO remove test data\n",
    "# Get test and validation accuracy for each model\n",
    "punc_acc_data = punc_data.drop(punc_data.columns.difference(['model_name', exp_param, 'val_acc', 'test_acc']), axis=1)\n",
    "punc_acc_data = punc_acc_data.rename(columns={'val_acc': 'Val Acc', 'test_acc': 'Test Acc'})\n",
    "punc_acc_data = punc_acc_data.melt(id_vars=['model_name', exp_param])\n",
    "\n",
    "g, fig = plot_facetgrid(punc_acc_data, x=\"model_name\", y=\"value\", hue=exp_param, col='variable', kind='swarm_violin',\n",
    "                            num_legend_col=1, y_label='Accuracy', x_label='Use Punctuation',\n",
    "                            share_y=False, num_col=1, dodge=True, colour='default')\n",
    "# g.savefig(os.path.join(task_name, exp_param ,exp_param + '_accuracy.png'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test assumptions using Shapiro-Wilks test for normality and Levene test for equal variance (Homoscedasticity)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the metric to test and statistical significance level\n",
    "metric = 'val_acc'\n",
    "\n",
    "# Run Shaprio-wilks for all models and punctuation flag value\n",
    "shapiro_wilk_test(punc_data, exp_param, metric)\n",
    "\n",
    "# Run Levene test for equal variance\n",
    "levene_test(punc_data, exp_param, metric)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform t-test comparing punctuation true and false for each model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run t-test\n",
    "t_test(punc_data, exp_param, metric)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Input Sequence Combinations\n",
    "\n",
    "Compare minimum and maximum vocabulary size (5k or 10k words), sequence lengths (50 or 128 tokens) and use of punctuation.\n",
    "\n",
    "Combination | Punc True | Punc False\n",
    "----------- | :-------: | :--------:\n",
    "Seq=50 Voc=5k   | [x]       | [x]\n",
    "Seq=128 Voc=5k  | [x]       | [x]\n",
    "Seq=50 Voc=10k  | [x]       | [x]\n",
    "Seq=128 Voc=10k | [x]       | [x]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Name of the experiment parameters for comparison\n",
    "exp_params = ['vocab_size', 'max_seq_length', 'use_punct']\n",
    "# Load punctuation experiment data\n",
    "inseq_data = load_dataframe(os.path.join(task_name, 'input_seq', 'input_seq_data.csv'))\n",
    "inseq_mean_data = load_dataframe(os.path.join(task_name, 'input_seq', 'input_seq_mean_data.csv'))\n",
    "\n",
    "# Get the max values for each model\n",
    "print(\"Best validation accuracy in mean data:\")\n",
    "print(inseq_mean_data.loc[[inseq_mean_data['val_acc'].idxmax()], ['model_name'] + exp_params + ['val_acc']])\n",
    "print(\"Best test accuracy in mean data:\")\n",
    "print(inseq_mean_data.loc[[inseq_mean_data['test_acc'].idxmax()], ['model_name'] + exp_params + ['test_acc', 'f1_micro']])\n",
    "inseq_data_max = get_max(inseq_mean_data, exp_params)\n",
    "inseq_data_max"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test assumptions using Shapiro-Wilks test for normality and Levene test for equal variance (Homoscedasticity)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add column to identify each combination\n",
    "inseq_data = inseq_data.drop(inseq_data.columns.difference(['model_name', 'val_acc', 'use_punct', 'vocab_size', 'max_seq_length']), axis=1)\n",
    "conditions = [(inseq_data['use_punct'] == True) &(inseq_data['vocab_size'] == 5000) & (inseq_data['max_seq_length'] == 50),\n",
    "              (inseq_data['use_punct'] == True) &(inseq_data['vocab_size'] == 5000) & (inseq_data['max_seq_length'] == 128),\n",
    "              (inseq_data['use_punct'] == True) &(inseq_data['vocab_size'] == 10000) & (inseq_data['max_seq_length'] == 50),\n",
    "              (inseq_data['use_punct'] == True) &(inseq_data['vocab_size'] == 10000) & (inseq_data['max_seq_length'] == 128),\n",
    "              (inseq_data['use_punct'] == False) &(inseq_data['vocab_size'] == 5000) & (inseq_data['max_seq_length'] == 50),\n",
    "              (inseq_data['use_punct'] == False) &(inseq_data['vocab_size'] == 5000) & (inseq_data['max_seq_length'] == 128),\n",
    "              (inseq_data['use_punct'] == False) &(inseq_data['vocab_size'] == 10000) & (inseq_data['max_seq_length'] == 50),\n",
    "              (inseq_data['use_punct'] == False) &(inseq_data['vocab_size'] == 10000) & (inseq_data['max_seq_length'] == 128),]\n",
    "choices = ['P=t_V=5k_S=50', 'P=t_V=5k_S=128', 'P=t_V=10k_S=50', 'P=t_V=10k_S=128',\n",
    "           'P=f_V=5k_S=50', 'P=f_V=5k_S=128', 'P=f_V=10k_S=50', 'P=f_V=10k_S=128']\n",
    "inseq_data['exp_params'] = np.select(conditions, choices)\n",
    "\n",
    "# Set the metric to test and new exp_param column\n",
    "metric = 'val_acc'\n",
    "exp_param = 'exp_params'\n",
    "\n",
    "# Run Shaprio-wilks for all models and punctuation flag value\n",
    "shapiro_wilk_test(inseq_data, exp_param, metric)\n",
    "\n",
    "# Run Levene test for equal variance\n",
    "levene_test(inseq_data, exp_param, metric)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform ANOVA for each model, comparing different sequence length and vocabulary size groups,\n",
    "followed by Tukey Honest Significant Difference Post-Hoc analysis for pairwise comparison of groups.\n",
    "\n",
    "Heatmaps shows Tukey-HSD results for each model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run ANOVA\n",
    "one_way_anova_test(inseq_data, exp_param, metric)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run Tukey-HSD post-hoc analysis\n",
    "tukey_frame = tukey_hsd(inseq_data, exp_param, metric, show_result=False)\n",
    "\n",
    "# Drop the un-needed columns and generate heatmaps\n",
    "tukey_frame = tukey_frame.drop(columns=['meandiff', 'lower', 'upper', 'reject'], axis=1)\n",
    "\n",
    "g, fig = plot_facetgrid(tukey_frame, x='group1', y='group2', hue='p-value', col='model_name', kind='heatmap',\n",
    "                        title='', y_label='', x_label='', num_col=2, colour='RdBu_r',\n",
    "                        annot=True, fmt='0.3', linewidths=0.5, cbar=False, custom_boundaries=[0.0, 0.05, 1.0],\n",
    "                        y_tick_rotation=0, x_tick_rotation=45, height=4)\n",
    "# g.savefig(os.path.join(task_name, exp_param ,exp_param + '_anova.png'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO Word Vectors\n",
    "\n",
    "Compare different dimensions of Word2Vec, GloVe, Fasttext, Numberbatch and Dependency (and random?) for each model.\n",
    "Dimension increments of 50 in range [100, 300]."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Name of the experiment parameter for comparison\n",
    "exp_params = ['embedding_type', 'embedding_dim']\n",
    "\n",
    "# Load language models experiment data\n",
    "embed_data = load_dataframe(os.path.join(task_name, 'embedding_type', 'embedding_type_data.csv'))\n",
    "embed_mean_data = load_dataframe(os.path.join(task_name,  'embedding_type', 'embedding_type_mean_data.csv'))\n",
    "\n",
    "# Get the max values for each model\n",
    "print(\"Best validation accuracy in mean data:\")\n",
    "print(embed_mean_data.loc[[embed_mean_data['val_acc'].idxmax()], ['model_name'] + exp_params + ['val_acc']])\n",
    "print(\"Best test accuracy in mean data:\")\n",
    "print(embed_mean_data.loc[[embed_mean_data['test_acc'].idxmax()], ['model_name'] + exp_params + ['test_acc', 'f1_micro']])\n",
    "embed_data_max = get_max(embed_mean_data, exp_params)\n",
    "embed_data_max"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot accuracy of each model per embedding type and dimension."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO remove test data\n",
    "# Get test and validation accuracy for each model\n",
    "embed_acc_data = embed_mean_data.drop(embed_mean_data.columns.difference(['model_name'] + exp_params + ['val_acc', 'test_acc']), axis=1)\n",
    "\n",
    "g, fig = plot_facetgrid(embed_acc_data, x='embedding_type', y='embedding_dim', hue='val_acc', col='model_name', kind='heatmap',\n",
    "                        title='', y_label='', x_label='', num_col=2, colour='YlGnBu',\n",
    "                        annot=True, fmt='0.3', linewidths=0.5, cbar=False, height=4)\n",
    "# g.savefig(os.path.join(task_name, exp_param ,exp_param + '_accuracy.png'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test ANOVA assumptions using Shapiro-Wilks test for normality and Levene test for equal variance (Homoscedasticity)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the metric to test\n",
    "metric = 'val_acc'\n",
    "\n",
    "for embed_type in embed_data['embedding_type'].unique():\n",
    "    data = embed_data.loc[embed_data['embedding_type'] == embed_type]\n",
    "    print(\"Embedding Type: \" + embed_type)\n",
    "    # Run Shaprio-wilks for all models and sequence lengths\n",
    "    shapiro_wilk_test(data, 'embedding_dim', metric)\n",
    "\n",
    "    # Run Levene test for equal variance\n",
    "    levene_test(data, 'embedding_dim', metric)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform Two-way ANOVA for each model, comparing different embedding type and dimension groups,\n",
    "followed by Tukey Honest Significant Difference Post-Hoc analysis for pairwise comparison of groups.\n",
    "\n",
    "Heatmaps shows Tukey-HSD results for each model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run Two-way ANOVA\n",
    "two_way_anova_test(embed_data, 'embedding_dim', 'embedding_type', metric, show_result=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add embedding type and dimension column\n",
    "embed_data['embedding'] = embed_data.apply(lambda row: row['embedding_type'] + \"_\" + str(row['embedding_dim']), axis=1)\n",
    "\n",
    "# Run Tukey-HSD post-hoc analysis\n",
    "tukey_frame = tukey_hsd(embed_data, 'embedding', metric, show_result=False)\n",
    "\n",
    "# Drop the un-needed columns and generate heatmaps\n",
    "tukey_frame = tukey_frame.drop(columns=['meandiff', 'lower', 'upper', 'reject'], axis=1)\n",
    "\n",
    "g, fig = plot_facetgrid(tukey_frame, x='group1', y='group2', hue='p-value', col='model_name', kind='heatmap',\n",
    "                        title='', y_label='', x_label='', num_col=2, colour='RdBu_r',\n",
    "                        annot=True, fmt='0.3', linewidths=0.5, cbar=False, custom_boundaries=[0.0, 0.05, 1.0],\n",
    "                        y_tick_rotation=0, x_tick_rotation=45, height=4)\n",
    "# g.savefig(os.path.join(task_name, exp_param ,exp_param + '_anova.png'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO Attentional / multi-layer models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Language Models\n",
    "\n",
    "Using pre-trained language models to generate sequence representations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Name of the experiment parameter for comparison\n",
    "exp_param = 'embedding_type'\n",
    "\n",
    "# Load language models experiment data\n",
    "lm_data = load_dataframe(os.path.join(task_name, 'language_models', 'language_models_data.csv'))\n",
    "lm_mean_data = load_dataframe(os.path.join(task_name, 'language_models', 'language_models_mean_data.csv'))\n",
    "\n",
    "# Get the max values for each model\n",
    "print(\"Best validation accuracy in mean data:\")\n",
    "print(lm_mean_data.loc[[lm_mean_data['val_acc'].idxmax()], ['model_name', 'val_acc']])\n",
    "print(\"Best test accuracy in mean data:\")\n",
    "print(lm_mean_data.loc[[lm_mean_data['test_acc'].idxmax()], ['model_name', 'test_acc', 'f1_micro']])\n",
    "lm_data_max = get_max(lm_mean_data, [exp_param])\n",
    "lm_data_max.drop('embedding_type', axis=1, inplace=True)\n",
    "lm_data_max"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot accuracy of each language model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO remove test data\n",
    "# Get test and validation accuracy for each model\n",
    "lm_acc_data = lm_data.drop(lm_data.columns.difference(['model_name', exp_param, 'val_acc', 'test_acc']), axis=1)\n",
    "lm_acc_data = lm_acc_data.rename(columns={'val_acc': 'Val Acc', 'test_acc': 'Test Acc'})\n",
    "lm_acc_data = lm_acc_data.melt(id_vars=['model_name', exp_param])\n",
    "\n",
    "g, fig = plot_facetgrid(lm_acc_data, x=\"model_name\", y=\"value\", hue=\"model_name\", col='variable', kind='swarm_violin',\n",
    "                        num_legend_col=5, y_label='Accuracy', x_label='Language Model',\n",
    "                        share_y=False, num_col=1, colour='Paired', dodge=False)\n",
    "\n",
    "# g.savefig(os.path.join(task_name, exp_param ,exp_param + '_accuracy.png'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}